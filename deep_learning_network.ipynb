{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# The Lab  - Antwerp Maritime Academy\n",
    "\n",
    "## Introduction\n",
    "Our team was tasked with analyzing the data of sensor measurements from an engine room and other parts of a container ship. These sensors were measuring the presence and/or quantity of different chemical substances like CO and NO2. The task wasn't all that clear but there was one semi-clear objective: “gain insights from the measurements”. There were immediately a few techniques that came to mind.\n",
    "\n",
    "## Goal\n",
    "The ultimate goal for this part is to predict certain chemical substances based on 1 reading of a substance, thus reducing the number of sensors needed. Given a prediction of these substances, an assessment can be made to control the engine room for failure or maintenance.\n",
    "\n",
    "## Techniques\n",
    "The techniques that will be discussed are not all implemented in the final product. Although they were not all implemented, they were all researched and have a valid reason why we did not go through with the implementation. All technologies discussed have led to the final product and its reasoning behind it.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Researched but not implemented\n",
    "### Big Data\n",
    "The first problem present was data. We needed a lot of data to gain useful insights into the air quality of the engine room and other different parts of the ship. This is when we came to big data. The idea was that we set up a pipeline and handled our data through technology like Hadoop and Google Cloud Dataflow. This could allow us to use clean data to generate clear, non-polluted graphs and not worry about data augmentation or different biases. There were several reasons we did not go through with this. First of all, was the connection to Google Cloud a problem, we were not sure if it was possible to connect to the cloud from a moving ship. Even if it was possible, the cost of being constantly connected was most likely not worth the extra benefits of having near-perfect data fed to our algorithms. The second remark was on Hadoop. Hadoop is amazing just not for our use case. We were not going to feed the algorithms gigabytes of data per minute so there wasn’t a valid reason for us to use something as overkill as Hadoop. This is why we did not bother implementing such technologies, not that it would have cost us performance or added value. It just wouldn’t have been necessary and a waste of time.\n",
    "\n",
    "### Data pipeline\n",
    "An aspect of the big data implementation concept was the data pipeline. This builds upon the continuous data stream idea. Although the whole POC was not implemented, the use case for a data pipeline was clear. Data could flow right into simple visualizations of different chemical substances and be shown as a barometer to keep the personnel onboard updated about potentially harmful situations due to alarmingly high concentrations of certain substances for a certain amount of time. Later on, this idea was taken over by a teammate of mine to be implanted in their part of the project, is used to visualize real-time data.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Used articles\n",
    "- [How to Train Deep Neural Networks Over Data Streams](https://towardsdatascience.com/how-to-train-deep-neural-networks-over-data-streams-fdab15704e66)\n",
    "- [Deep Neural Networks for Regression Problems](https://towardsdatascience.com/deep-neural-networks-for-regression-problems-81321897ca33)\n",
    "- [How to Develop Multi-Output Regression Models with Python](https://machinelearningmastery.com/multi-output-regression-models-with-python/)\n",
    "- [Deep Learning Models for Multi-Output Regression](https://machinelearningmastery.com/deep-learning-models-for-multi-output-regression/)\n",
    "- [Multi-output Regression Example with Keras Sequential Model](https://www.datatechnotes.com/2019/12/multi-output-regression-example-with.html)\n",
    "- [A Gentle Introduction to k-fold Cross-Validation](https://machinelearningmastery.com/k-fold-cross-validation/)\n",
    "- [Various Optimization Algorithms For Training Neural Network](https://towardsdatascience.com/optimizers-for-training-neural-network-59450d71caf6)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Implementation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from statsmodels.tsa.stattools import grangercausalitytests\n",
    "from scipy.stats import pearsonr\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "import pandas as pd"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## data selection\n",
    "\n",
    "The data gets selected based on the needs of the network. We choose a dataframe with the input of the network and a dataframe with the output of the network. Later we separate the data using the k-folds cross validation algorithm to ensure the model will not over-fit."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_with_dates = pd.read_csv(\"./csv/engine_room.csv\").dropna().to_numpy()\n",
    "df = df_with_dates[:, 2:15].astype('float32')\n",
    "\n",
    "df_co_in = df[:, 7].astype('float32')\n",
    "df_co_out = df[:, (2, 4, 9)].astype('float32')\n",
    "\n",
    "df_pm_in = df[:, 10].astype('float32')\n",
    "df_pm_out = df[:, (11, 12)].astype('float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Correlation test\n",
    "\n",
    "To predict something based on something else, you need to know whether these two have something to do with each other. This is where correlation comes into play. To be more precise, Pearson’s correlation. This measures the correlation coefficient that describes the strength of the linear relationship between two data points. If it is close to 1 or -1, the relationship is strong. We tested all correlations between the given substances in our dataset and picked the strong correlations (> 0.7 or < -0.7). This gave us the certainty that what we were predicting was going to be the real (or close to) value if we were to measure it. (keep in mind that all correlations are going to be double since it goes over each relation both ways)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "elements_as_array = np.array(\n",
    "    ['T', 'RH', 'CO2', 'NO2', 'O3', 'NO', 'SO2', 'CO', 'H2S', 'TVOC', 'PM1', 'PM2.5', 'PM10']\n",
    ")\n",
    "\n",
    "for i in range(df.shape[1]):\n",
    "    for j in range(df.shape[1]):\n",
    "        if i != j:\n",
    "            cor, _ = pearsonr(df[:, i], df[:, j])\n",
    "            if cor < -0.7 or cor > 0.7:\n",
    "                print(f'corr between element {elements_as_array[i]} and element {elements_as_array[j]}: {cor}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "There seems to be a correlation between some chemical substances. The goal right now is to create 2 models that predicts **TVOC**, **O3** and **CO2** based on **CO** readings and one that predicts **PM2.5** and **PM10** based on **PM1**. This could be useful to achieve since we then only need to install a **CO** and a **PM1** sensor to get relevant information about these components."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Causality test\n",
    "\n",
    "A causality test defines or rejects a hypothesis that change in y substance (in our case) is caused by substance x. We can reject the hypothesis when the p-value is lower than 0.05. This would suggest that that substance x does indeed influence the value of subtance y.\n",
    "\n",
    "h1:  x-values do explain the variation in y\n",
    "h0:  x-values do not explain the variation in y"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "co_and_tvoc = df[:, [7, 2]]\n",
    "co_and_o3 = df[:, [7, 4]]\n",
    "co_and_co2 = df[:, [7, 9]]\n",
    "pm1_and_pm2 = df[:, [10, 11]]\n",
    "pm1_and_pm10 = df[:, [10, 12]]\n",
    "\n",
    "grangercausalitytests(co_and_tvoc, maxlag=3)\n",
    "grangercausalitytests(co_and_o3, maxlag=3)\n",
    "grangercausalitytests(co_and_co2, maxlag=3)\n",
    "grangercausalitytests(pm1_and_pm2, maxlag=3)\n",
    "grangercausalitytests(pm1_and_pm10, maxlag=3)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Looking at the p-values we can indeed see that **CO** causes the other substances to change. This discovery leads us to believe that the h1 is indeed true and that h0 can be rejected. The same goes for **PM1** and its counterparts."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Deep Learning\n",
    "Deep learning is often used as a buzzword but also as a means to predict and classify. This is where we got the inspiration to use a neural network to predict certain substances based on one substance reading. This is something I implemented because if you were able to predict the values of some substances, it means you would not have to install an extra sensor. This could mean several things, firstly it could save time installing and maintaining the sensors, time that can now be spent on other time-consuming activities that maybe require more attention. Secondly, if you can predict something, you don’t have to measure it with a sensor. This could mean that the ships could have fewer sensors onboard, which would save money. Furthermore, fewer sensors mean less parts that can break. This is why softsensors are the way to go in my eyes.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the neural network\n",
    "\n",
    "This will be a multilayer perceptron model, used for our multi-output regression. We define a model with an input as big as the amount of substances present in the dataframe. Then we add some hidden layers to calculate the right weights for each neuron, this amount of hidden layers is a result of a lot of trail and error. The output will consist of 3 neurons for model_co and 2 neurons for model_pm, representing the 2 or 3 substances we want to predict."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Multi-output regression\n",
    "Linear regression is a relatively simple and well know the statistical model that assumes there is a linear relationship between a single input variable (x) and a single output variable (y). This is where things get sticky. In the dataset used, predictions are going to be made for multiple output variables based on one input variable. This is where multi-output regression comes in. The concept is very similar, the one of the major differences is that there is going to be a neural network. This network allows us to predict multiple outputs based on a single input. Of course, like any deep learning model, the network needs to be trained before any prediction can be done. After having defined the data sets, the challenge was to find the right kind of activation, loss, and optimizer. The best activation function turned out to be the relatively standard relu function."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model_co = Sequential([\n",
    "    Dense(20, input_dim=1, kernel_initializer='he_uniform', activation='relu'),\n",
    "    Dense(10, activation='relu'),\n",
    "    Dense(50, activation='relu'),\n",
    "    Dense(3)\n",
    "])\n",
    "\n",
    "model_pm = Sequential([\n",
    "    Dense(20, input_dim=1, kernel_initializer='he_uniform', activation='relu'),\n",
    "    Dense(10, activation='relu'),\n",
    "    Dense(5, activation='relu'),\n",
    "    Dense(2)\n",
    "])\n",
    "\n",
    "model_co.compile(loss='mae', optimizer='adam')\n",
    "model_pm.compile(loss='mae', optimizer='adam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the model\n",
    "\n",
    "Here we are going to evaluate the model and return the mean absolute error to see how good our model did with on the data we provided. The MAE gives us the average of how far off the network guessed relative to the actual readings. To train the model k-folds cross validation will be used, this is an algorithm that splits the data up in k amount of parts. If we were to split the data up in 6 parts, it would be called 6-fold. K-fold is used to counter over-fitting, a phenomenon in neural networks that is a result of a network not being able to generalize. K-fold works by dividing the data in k amount of parts, while the network trains, the parts are being rotated so that it never trains with the same data for a longer period of time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "results_co = list()\n",
    "n_input, n_output = 1, 3\n",
    "cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "\n",
    "for train, test in cv.split(df):\n",
    "    data_test, data_train = df_co_in[train], df_co_in[test]\n",
    "    output_test, output_train = df_co_out[train], df_co_out[test]\n",
    "    model_co.fit(data_train, output_train, verbose=0, epochs=500)\n",
    "    mae_co = model_co.evaluate(data_test, output_test, verbose=1)\n",
    "    results_co.append(mae_co)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "results_pm = list()\n",
    "n_input, n_output = 1, 2\n",
    "cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "\n",
    "for train, test in cv.split(df):\n",
    "    data_test, data_train = df_pm_in[train], df_pm_in[test]\n",
    "    output_test, output_train = df_pm_out[train], df_pm_out[test]\n",
    "    model_pm.fit(data_train, output_train, verbose=0, epochs=500)\n",
    "    mae_pm = model_pm.evaluate(data_test, output_test, verbose=1)\n",
    "    results_pm.append(mae_pm)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantification of the results\n",
    "\n",
    "To get a better idea of the results, the MAE and standard deviation are calculated for the whole network. This represents again how far the network was off and what the average deviation is of the mean. (lower is better)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "print('(co model) mean absolute error: %.3f standard deviance: %.3f' % (mean(results_co), std(results_co)))\n",
    "print('(pm model) mean absolute error: %.3f standard deviance: %.3f' % (mean(results_pm), std(results_pm)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Prediction\n",
    "\n",
    "Now that the model is trained, the next logical step is to predict values. The following will display a prediction of **TVOC**, **O3** and **CO2** based on only **CO** values and **PM2.5** and **PM10** based on **PM1**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "new_co_reading = [452.19]\n",
    "prediction_co = model_co.predict(new_co_reading)\n",
    "#prediction order: CO2, O3, TVOC\n",
    "print(f'Predicted: %s' % prediction_co)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "new_pm_reading = [1.63]\n",
    "prediction_pm = model_pm.predict(new_pm_reading)\n",
    "#prediction order: pm2.5, pm10\n",
    "print(f'Predicted: %s' % prediction_pm)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Results\n",
    "The predictions made by the regression model can be used for assessing the state of the engine. As previous research shows us that there is a causation effect to the rising of CO, TVOC, and CO2. Given the engine used is a diesel engine and the CO readings are going up, there is most likely servicing that needs to be done to the engine. The fact is that bad combustion can lead to the burning of lubrication oil resulting in the creation of several substances which also include CO, TVOC, and others.\n",
    "\n",
    "## Possible further additions\n",
    "If the project had a longer lifespan, the first thing that I would do is add gradient boosting. this is a method that is commonly used to enhance the performance of regression problems, classification problems... Secondly I would feed more data to the model. of course, this is a common thing to do with any machine learning model. Feeding more data into the model would certainly drop the loss even further. Another interesting thing I'd like to do is add more sensors for a while to see if any more substances could be predicted using the same model. I would also reverse the model to a multi-input regression model to predict substances that could become a safety concern to the crew and could be detected by combining the readings of different substances."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## source reference\n",
    "Brownlee, J. (2020, August 27). Deep Learning Models for Multi-Output Regression. Machine Learning Mastery. Retrieved February 18, 2022, from https://machinelearningmastery.com/deep-learning-models-for-multi-output-regression/\n",
    "Brownlee, J. (2021, April 26). How to Develop Multi-Output Regression Models with Python. Machine Learning Mastery. Retrieved February 18, 2022, from https://machinelearningmastery.com/multi-output-regression-models-with-python/\n",
    "Kozyrkov, C. (2021, December 15). What is correlation? - Towards Data Science. Medium. Retrieved February 13, 2022, from https://towardsdatascience.com/what-is-correlation-975ea899aaed\n",
    "Kozyrkov, C. (2021a, December 7). Statistical inference in one sentence - HackerNoon.com. Medium. Retrieved February 13, 2022, from https://medium.com/hackernoon/statistical-inference-in-one-sentence-33a4683a6424\n",
    "Brownlee, J. (2020a, August 20). How to Calculate Correlation Between Variables in Python. Machine Learning Mastery. Retrieved February 18, 2022, from https://machinelearningmastery.com/how-to-use-correlation-to-understand-the-relationship-between-variables/\n",
    "Brownlee, J. (2020a, August 15). Linear Regression for Machine Learning. Machine Learning Mastery. Retrieved February 19, 2022, from https://machinelearningmastery.com/linear-regression-for-machine-learning/\n",
    "Diesel and Gasoline Engine Exhausts. (n.d.). NCBI. Retrieved March 11, 2022, from https://www.ncbi.nlm.nih.gov/books/NBK531294/#:%7E:text=Incomplete%20combustion%20results%20in%20the,the%20fuel%20and%20lubricating%20oil.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}